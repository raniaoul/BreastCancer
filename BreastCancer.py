# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r-U3qc-pxj75298MX7a2cyc0TPoYOVop
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score
from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, KFold, cross_val_score
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from six import StringIO
from IPython.display import Image
import pydotplus
from scipy.cluster import hierarchy

feat_all,label_all=load_breast_cancer(return_X_y=True,as_frame=True)
feat_all.isnull().sum()

corr_fig1,ax1=plt.subplots(1,1,figsize=(10, 8),)
# Compute pair wise correlations
corr=feat_all.corr().values
# Compute the hierarchical clustering tree
link=hierarchy.ward(corr)
dendro=hierarchy.dendrogram(link,labels=feat_all.columns,ax=ax1,leaf_rotation=90,leaf_font_size=10)
dendro_index=np.arange(0,len(dendro["ivl"]))
corr_fig1.tight_layout()
# Color list of the four feature clusters
color_list=["red","navy", "black", "green"]
# Fix a level of four in the HC tree to determine feature clusters
clusterlevel=4
# Determine the id cluster list
clusterid_list= hierarchy.fcluster(link,clusterlevel,criterion='distance')
# This dictionary will contain the list of features for each
# cluster
featurecluster_dict = dict()
for idx, clusterid in enumerate(clusterid_list):
 if clusterid not in featurecluster_dict.keys():
     featurecluster_dict[clusterid]=dict()
     featurecluster_dict[clusterid]["numfeat"]=[]
     featurecluster_dict[clusterid]["namefeat"]=[]
 featurecluster_dict[clusterid]["color"]=color_list[clusterid-1]
 featurecluster_dict[clusterid]["numfeat"].append(idx)
 featurecluster_dict[clusterid]["namefeat"].append(feat_all.columns[idx])
def preprocess(trainrate=1.0):
    # Load the data and get the name of all features
    feat_all,label_all=load_breast_cancer(return_X_y=True,as_frame=True)
    featnames=np.array(feat_all.columns)
    # Shuffle the data
    data_all=pd.concat([feat_all,label_all],axis=1).sample(frac=1)
    label_all=data_all["target"]
    feat_all=data_all.drop("target",axis=1)
    # Get normalized features
    StdSc=StandardScaler()
    StdSc.fit(feat_all)
    featN_all=StdSc.transform(feat_all)
    featN_all=pd.DataFrame(featN_all,columns=feat_all.columns)

    # Split between training and testing sets
    trainsize=int(trainrate*len(feat_all.index))
    feat_train=feat_all[:trainsize]
    featN_train=featN_all[:trainsize]
    label_train=label_all[:trainsize]
    feat_test=feat_all[trainsize:]
    featN_test=featN_all[trainsize:]
    label_test=label_all[trainsize:]
    normmean_arr=StdSc.mean_
    normstd_arr=(StdSc.var_)**0.5
    return feat_all,label_all,featnames,featN_all,feat_train,featN_train,feat_test,featN_test,label_train,label_test,normmean_arr,normstd_arr

def trainGbc(params,feat_train,label_train,feat_test,label_test,setbestestim=False,setfeatimp=False,featurecluster_dict=None):
    # If the best number of estimators has to be determined
    if setbestestim:
        Gbc=GradientBoostingClassifier(**params)
        Gbc.fit(feat_train,label_train)
        # Determine the best n_estimators
        scoretest_list=[]
        scoretrain_list=[]

        # Compute accuracy scores for training and testing with
        # different n_estimators
        for pred_test in Gbc.staged_predict(feat_test):
            scoretest=accuracy_score(label_test,pred_test)
            scoretest_list.append(scoretest)
        for pred_train in Gbc.staged_predict(feat_train):
            scoretrain=accuracy_score(label_train,pred_train)
            scoretrain_list.append(scoretrain)
        # Plot the figure showing the training and testing
        # accuracies' evolution with n_estimators
        nestim_fig,ax=plt.subplots(1,1,figsize=(10,8),)
        plt.plot(np.arange(params["n_estimators"]),scoretrain_list,label="Train")
        plt.plot(np.arange(params["n_estimators"]),scoretest_list,label="Test")
        plt.legend()
        plt.xlabel("n_estimators")
        plt.ylabel("Accuracy")
        nestim_fig.savefig("nestim.pdf")
        plt.show()

    # Cross validate and fit a GBTC estimator
    else:
        Gbc=GradientBoostingClassifier(**params)
        score=cross_validate(Gbc,feat_train,label_train,cv=5,scoring="accuracy")
        print("Gbc Cross Validation Accuracy (Testing)")
        print(np.mean(score["test_score"]))
        Gbc.fit(feat_train,label_train)
    # Determine feature importance
    if setfeatimp:
        impfeat_list=Gbc.feature_importances_
        indexsort=np.argsort(Gbc.feature_importances_)
        impfeat_fig,ax=plt.subplots(1,1,figsize=(10,8),)
        pos=np.arange(len(indexsort))+0.5
        plt.barh(pos,impfeat_list[indexsort])
        plt.yticks(pos,np.array(feat_train.columns)
            [indexsort],fontsize=10,color="red")
        # If feature clustering, color the features depending on
        # their clusters
        if featurecluster_dict!=None:
            for ifeat,featname in enumerate(np.array(feat_train.columns)[indexsort]):
                for clusterkey in featurecluster_dict.keys():
                    if featname in featurecluster_dict[clusterkey]["namefeat"]:
                        ax.get_yticklabels()[ifeat].set_color(featurecluster_dict[clusterkey]["color"])
            plt.xlabel("Importance")
            plt.xscale("log")
            plt.xticks(size=10)
            impfeat_fig.savefig("impfeat.pdf")
            plt.show()
    return Gbc

# Determine feature importance with n_estimator=300
params={"n_estimators":300,"learning_rate":0.01,"min_samples_split":5,"max_depth":4}
feat_all,label_all,featnames,featN_all,feat_train,featN_train,feat_test,featN_test,label_train,label_test,mean_feat,std_feat=preprocess(trainrate=1.0)
Gbc=trainGbc(params,feat_train,label_train,feat_test,label_test,setbestestim=False,setfeatimp=True,featurecluster_dict=featurecluster_dict)

# Charger et diviser les données
breast_cancer = datasets.load_breast_cancer()
X = breast_cancer.data
y = breast_cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Lancer l'apprentissage
clf = DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)

# Tester
y_pred = clf.predict(X_test)

# Afficher les résultats
result = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(result)
result1 = classification_report(y_test, y_pred)
print("Classification Report:")
print(result1)
result2 = accuracy_score(y_test, y_pred)
print("Accuracy:", result2)

# Plot Graph
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True, feature_names=breast_cancer.feature_names, class_names=breast_cancer.target_names)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('breast_cancer_Tree.png')
Image(graph.create_png())

# Charger les données
breast_cancer = datasets.load_breast_cancer()
column_names = ['worst radius', 'worst concave points']

# Identifier les index des colonnes dans le tableau de données
column_indices = [list(breast_cancer.feature_names).index(name) for name in column_names]

# Extraire les colonnes spécifiques du tableau de données
X = breast_cancer.data[:, column_indices]

y = breast_cancer.target

# Définir les valeurs minimales et maximales des axes
x_min, x_max = 5, X[:, 0].max() + 1
y_min, y_max = -0.05, X[:, 1].max() + 0.05  # Ajuster la valeur maximale pour inclure les graduations souhaitées

# Définir les graduations des axes
x_ticks = np.arange(x_min, x_max, 5)  # Par exemple, une graduation tous les 10
y_ticks = np.arange(y_min, y_max, 0.05)  # Graduation de 0.05 pour l'axe y

# Générer la grille de points
h = 0.05
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
X_plot = np.c_[xx.ravel(), yy.ravel()]

# Apprentissage
svc_classifier = svm.SVC(kernel='linear', C=1.0).fit(X, y)
Z = svc_classifier.predict(X_plot)
Z = Z.reshape(xx.shape)

# Personnalisation du graphe d'affichage
plt.figure(figsize=(14, 5))
plt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.1)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)

# Définir les graduations des axes x et y
plt.xticks(x_ticks)
plt.yticks(y_ticks)

plt.xlabel('Worst Radius')
plt.ylabel('Worst Concave Points')
plt.title('Support Vector Classifier with linear kernel')

plt.show()

# Charger les données du cancer du sein
breast_cancer = load_breast_cancer()
X = breast_cancer.data
feature_names = breast_cancer.feature_names
print("Features used for clustering:", feature_names)


plt.scatter(X[:, 0], X[:, 1], s=20);
plt.show()
# Appliquer l'algorithme K-means
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)

# Prédire les clusters
y_kmeans = kmeans.predict(X)

# Afficher les résultats du clustering
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50)
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)
plt.title('K-means Clustering on Breast Cancer Dataset')
plt.show()

# Charger les données du cancer du sein
data = load_breast_cancer()
X = data.data  # Features
y = data.target  # Target variable

# Créer un pipeline avec mise à l'échelle et régression logistique
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logistic', LogisticRegression(max_iter=200))  # Augmenter le nombre maximal d'itérations
])

# Diviser le dataset en 20 parties pour la validation croisée
kfold = KFold(n_splits=20)

# Obtenir les résultats de la validation croisée
results = cross_val_score(pipeline, X, y, cv=kfold)

# Afficher les résultats
print("Cross-validation results for Logistic Regression on Breast Cancer dataset:")
print(results)
print("Mean accuracy:", results.mean())
print("Standard deviation:", results.std())

import numpy as np
from sklearn.cluster import MeanShift
import matplotlib.pyplot as plt
from matplotlib import style
from sklearn.datasets import load_breast_cancer

style.use("ggplot")

# Charger le dataset du cancer du sein
breast_cancer = load_breast_cancer()
X = breast_cancer.data
feature_names = breast_cancer.feature_names
print("Features used for clustering:", feature_names)

# Affichage du dataset initial (en utilisant les deux premières features pour la visualisation)
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title('Breast Cancer Dataset (First Two Features)')
plt.show()

# Lancer l'apprentissage avec MeanShift
ms = MeanShift()
ms.fit(X)

# Affichage des centres des clusters
labels = ms.labels_
cluster_centers = ms.cluster_centers_
print("Cluster centers:\n", cluster_centers)

# Affichage des clusters
n_clusters_ = len(np.unique(labels))
print("Estimated number of clusters:", n_clusters_)

# Affichage des clusters en utilisant les deux premières features pour la visualisation
plt.figure(figsize=(10, 6))
colors = 10*['r.', 'g.', 'b.', 'c.', 'k.', 'y.', 'm.']
for i in range(len(X)):
    plt.plot(X[i][0], X[i][1], colors[labels[i]], markersize=5)
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='o', color='k', s=100, linewidths=5, zorder=10)
plt.title('MeanShift Clustering on Breast Cancer Dataset')
plt.show()

# Charger les données de cancer du sein
data = datasets.load_breast_cancer()
X = data.data
y = data.target
feature_names = data.feature_names

# Sélectionner uniquement les colonnes 'worst concave points' et 'worst radius'
selected_features = ['worst concave points', 'worst radius']
selected_indices = [list(feature_names).index(feature) for feature in selected_features]
X_selected = X[:, selected_indices]

# Diviser le jeu de données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)

# Créer et entraîner un modèle de régression linéaire pour chaque colonne de X
for i, feature_name in enumerate(selected_features):
    # Sélectionner la colonne courante de X
    X_col = X_test[:, i].reshape(-1, 1)
    X_train_col = X_train[:, i].reshape(-1, 1)

    # Créer et entraîner le modèle de régression linéaire
    model = linear_model.LinearRegression()
    model.fit(X_train_col, y_train)

    # Faire des prédictions sur l'ensemble de test
    y_pred = model.predict(X_col)

    # Évaluer les performances du modèle
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Caractéristique '{feature_name}': Erreur quadratique moyenne (MSE): {mse:.2f}, Coefficient de détermination (R²): {r2:.2f}")

    # Tracer les résultats
    plt.figure(figsize=(8, 6))
    plt.scatter(X_col, y_test, color='blue', label='Données de test')
    plt.plot(X_col, y_pred, color='red', linewidth=2, label='Prédictions')
    plt.xlabel(feature_name)
    plt.ylabel('Classe')
    plt.title(f'Régression linéaire sur la caractéristique "{feature_name}"')
    plt.legend()
    plt.show()

data = load_breast_cancer()
label_names = data['target_names']
labels = data['target']
feature_names = data['feature_names']
features = data['data']
gnb = GaussianNB()

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1)
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

from sklearn import datasets
from sklearn import linear_model
from sklearn import metrics
from sklearn.model_selection import train_test_split

# Charger le dataset du diabète
diabetes = datasets.load_breast_cancer()
X = diabetes.data  # Utiliser toutes les caractéristiques disponibles
y = diabetes.target

# Binariser la variable cible pour un problème de classification (utilisation d'une médiane comme seuil)
y_binary = (y > y.mean()).astype(int)

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.4, random_state=1)

# Instancier le modèle de régression logistique
logreg = linear_model.LogisticRegression(max_iter=200)

# Entraîner le modèle
logreg.fit(X_train, y_train)

# Tester le modèle
y_pred = logreg.predict(X_test)

# Afficher la matrice de confusion
print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, y_pred))

# Afficher la précision du modèle
print("Accuracy of Logistic Regression model is:", metrics.accuracy_score(y_test, y_pred) * 100)